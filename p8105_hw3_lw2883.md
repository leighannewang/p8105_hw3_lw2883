p8105\_hw3\_lw2883
================
Leighanne Wang
10/6/2020

## Problem 1

``` r
data("instacart")
```

This dataset contains **1384617 rows** and **15 columns**. Observations
are on the level of items in orders by user. There are user and order
variables such as user ID, order ID, order day, and order hour. There
are also item variables such as name, aisle, department, and some
numeric codes.

*How many aisles and which aisles are most items from?*

``` r
aisle_num =    
  instacart %>% 
    count(aisle) %>% 
    arrange(desc(n)) 
```

There are **134 aisles**. Most items are ordered from the Fresh
Vegetables, Fresh Fruits, and Packaged Vegetables Fruits aisles.

*Plot showing number of items ordered in each aisle (only aisles with
more than 10,000 items ordered)*

``` r
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_lw2883_files/figure-gfm/instcart_plot-1.png" width="90%" />

*Table showing 3 most popular items in each of these aisles: baking
ingredients, dog food care, packaged vegetables fruits*

``` r
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

*Table showing the mean hour of the day at which Pink Lady Apples
vs. Coffee Ice Cream are ordered on each day of the week*

``` r
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) 
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

## Problem 2

*Read the accelerometer dataset. Tidy data, create weekday/weekend
variable, encode variable classes:*

``` r
accel_df =
  read_csv("./data/accel_data.csv") %>% 
    janitor::clean_names() %>% 
    pivot_longer(
      activity_1:activity_1440,
      names_to = "minute",
      names_prefix = "activity_",
      values_to = "activity_counts") %>% 
    mutate(
      minute = as.double(minute),
      day_type = recode(day, "Saturday" = "weekend", "Sunday" = "weekend", "Monday" = "weekday", "Tuesday" = "weekday", "Wednesday" = "weekday", "Thursday" = "weekday", "Friday" = "weekday")
      )
```

This dataset contains data collected from an accelerometer of a 63 year
old male with a BMI of 25 and congestive heart failure. Variables
included in this dataset are week number, day (including day ID number
and type of day), minute, and activity count for each minute of a day.
There are **50400 rows** and **6 columns**.

*Aggregate across minutes to create total activity over the day and
create table to show totals*

``` r
accel_df %>% 
  group_by(week, day) %>% 
  summarize(
    total_activity = sum(activity_counts)
  ) %>%
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) 
```

    ## `summarise()` regrouping output by 'week' (override with `.groups` argument)

    ## # A tibble: 5 x 8
    ## # Groups:   week [5]
    ##    week  Friday  Monday Saturday Sunday Thursday Tuesday Wednesday
    ##   <dbl>   <dbl>   <dbl>    <dbl>  <dbl>    <dbl>   <dbl>     <dbl>
    ## 1     1 480543.  78828.   376254 631105  355924. 307094.   340115.
    ## 2     2 568839  295431    607175 422018  474048  423245    440962 
    ## 3     3 467420  685910    382928 467052  371230  381507    468869 
    ## 4     4 154049  409450      1440 260617  340291  319568    434460 
    ## 5     5 620860  389080      1440 138421  549658  367824    445366

**use factors to make day of the week in the right order**

Looking at the total activity over the days and weeks, we see that for
the most part total activity count ranges from 300,000 to around
600,000. What is noticeable is that the last two weekends in the dataset
have less total activity count than in previous weekends, especially
Saturday which has significantly less activity count than any other day.

  - minute xaxis and activity count on yaxis // color to map day of week
    // scatterplot with geom\_line to connect dots // describe patterns
    minute might be character which will turn into factor in graph \> so
    make sure it’s numeric

## Problem 3

  - separate to create year, month, day varaibles // make judgement call
    to see if you think the obs are in reasonable units // count
    snowfall for most commonly obs values
  - data manipulation then plotting step \> 2 panel one for Jan and one
    for Jul // organize data: groupby (station, year, and month) and
    summarize (average max temperature) // filter to get Jan + Jul
    either before or after groupby + summarize // plotting: average max
    temp for jan 1981,82,83.. (over years) for each station (use facet
    to get 2 panel) // structure: does it get warmer? are some stations
    always colder?
  - 2 panel plot \> since they’re completely different make 2 different
    plots then patchwork //contour plot, bin plot, hex plot for first
    plot // for second plot: filter then use distribution plot (boxplot,
    violin, ridge)
