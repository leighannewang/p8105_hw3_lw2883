---
title: "p8105_hw3_lw2883"
author: "Leighanne Wang"
date: "10/6/2020"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(haven)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r, instacart}
data("instacart")
```

This dataset contains **`r nrow(instacart)` rows** and **`r ncol(instacart)` columns**. Observations are on the level of items in orders by user. There are user and order variables such as user ID, order ID, order day, and order hour. There are also item variables such as name, aisle, department, and some numeric codes. 

*How many aisles and which aisles are most items from?*
```{r aisle_num}
aisle_num =    
  instacart %>% 
    count(aisle) %>% 
    arrange(desc(n)) 
```
There are **`r aisle_num %>% count()` aisles**. Most items are ordered from the **Fresh Vegetables, Fresh Fruits, and Packaged Vegetables Fruits aisles**.


*Plot showing number of items ordered in each aisle (only aisles with more than 10,000 items ordered)*
```{r, instcart_plot}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


*Table showing 3 most popular items in each of these aisles: baking ingredients, dog food care, packaged vegetables fruits*
```{r, instacart_table_popular}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```


*Table showing the mean hour of the day at which Pink Lady Apples vs. Coffee Ice Cream are ordered on each day of the week*
```{r, apples_icecream_table}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) 
```

## Problem 2

*Read the accelerometer dataset. Tidy data, create weekday/weekend variable, encode variable classes:*
```{r accel_df, message = FALSE}
accel_df =
  read_csv("./data/accel_data.csv") %>% 
    janitor::clean_names() %>% 
    pivot_longer(
      activity_1:activity_1440,
      names_to = "minute",
      names_prefix = "activity_",
      values_to = "activity_counts") %>% 
    mutate(
      minute = as.double(minute),
      day_type = recode(day, "Saturday" = "weekend", "Sunday" = "weekend", "Monday" = "weekday", "Tuesday" = "weekday", "Wednesday" = "weekday", "Thursday" = "weekday", "Friday" = "weekday")
      )
```

This dataset contains data collected from an accelerometer of a 63 year old male with a BMI of 25 and congestive heart failure. Variables included in this dataset are week number, day (including day ID number and type of day), minute, and activity count for each minute of a day. There are **`r nrow(accel_df)` rows** and **`r ncol(accel_df)` columns**. 



* aggregate across minutes to create total activity variable for each day > grouby (week then day or day id, then aggregate using mean or sum) + summarize // create table with 35 days with total activity count > make easier to read week number rows and day of the week across top // then describe any patterns
potential problems > use factors to make day of the week in the right order
* minute xaxis and activity count on yaxis // color to map day of week // scatterplot with geom_line to connect dots // describe patterns
minute might be character which will turn into factor in graph > so make sure it's numeric

## Problem 3
* separate to create year, month, day varaibles // make judgement call to see if you think the obs are in reasonable units // count snowfall for most commonly obs values
* data manipulation then plotting step > 2 panel one for Jan and one for Jul // organize data: groupby (station, year, and month) and summarize (average max temperature) // filter to get Jan + Jul either before or after groupby + summarize // plotting: average max temp for jan 1981,82,83.. (over years) for each station (use facet to get 2 panel) // structure: does it get warmer? are some stations always colder? 
* 2 panel plot > since they're completely different make 2 different plots then patchwork //contour plot, bin plot, hex plot for first plot // for second plot: filter then use distribution plot (boxplot, violin, ridge) 




