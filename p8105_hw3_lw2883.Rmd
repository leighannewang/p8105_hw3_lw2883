---
title: "p8105_hw3_lw2883"
author: "Leighanne Wang"
date: "10/6/2020"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(haven)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r, instacart}
data("instacart")
```

This dataset contains **`r nrow(instacart)` rows** and **`r ncol(instacart)` columns**. Observations are on the level of items in orders by user. There are user and order variables such as user ID, order ID, order day, and order hour. There are also item variables such as name, aisle, department, and some numeric codes. 

*How many aisles and which aisles are most items from?*
```{r aisle_num}
aisle_num =    
  instacart %>% 
    count(aisle) %>% 
    arrange(desc(n)) 
```
There are **`r aisle_num %>% count()` aisles**. Most items are ordered from the Fresh Vegetables, Fresh Fruits, and Packaged Vegetables Fruits aisles.


*Plot showing number of items ordered in each aisle (only aisles with more than 10,000 items ordered)*
```{r, instcart_plot}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


*Table showing 3 most popular items in each of these aisles: baking ingredients, dog food care, packaged vegetables fruits*
```{r, instacart_table_popular}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```


*Table showing the mean hour of the day at which Pink Lady Apples vs. Coffee Ice Cream are ordered on each day of the week*
```{r, apples_icecream_table, message = FALSE, collapse = TRUE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```

## Problem 2

*Read the accelerometer dataset. Tidy data, create weekday/weekend variable, encode variable classes:*
```{r accel_df, message = FALSE}
accel_df =
  read_csv("./data/accel_data.csv") %>% 
    janitor::clean_names() %>% 
    pivot_longer(
      activity_1:activity_1440,
      names_to = "minute",
      names_prefix = "activity_",
      values_to = "activity_counts") %>% 
    mutate(
      minute = as.double(minute),
      day_type = recode(day, "Saturday" = "weekend", "Sunday" = "weekend", "Monday" = "weekday", "Tuesday" = "weekday", "Wednesday" = "weekday", "Thursday" = "weekday", "Friday" = "weekday")
      )
```

This dataset contains data collected from an accelerometer of a 63 year old male with a BMI of 25 and congestive heart failure. Variables included in this dataset are week number, day (including day ID number and type of day), minute, and activity count for each minute of a day. There are **`r nrow(accel_df)` rows** and **`r ncol(accel_df)` columns**. 

*Aggregate across minutes to create total activity over the day and create table to show totals*
```{r, total_activity, message = FALSE, collapse = TRUE}
accel_df %>% 
  mutate(
    day = forcats::fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
    ) %>% 
  group_by(week, day) %>% 
  summarize(
    total_activity = sum(activity_counts)
  ) %>%
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
   knitr::kable()
```

Looking at the total activity over the days and weeks, we see that for the most part total activity count ranges from 300,000 to around 600,000. What is noticeable is that the last two weekends in the dataset have less total activity count than in previous weekends, especially Saturday which has significantly less activity count than any other day with only 1440 total activity count.


*Single-panel plot of 24 hour activity count from accelerometer data for each day*
```{r, accel_plot}
accel_df %>% 
  mutate( day = forcats::fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
    ) %>%
  ggplot(aes(x = minute, y = activity_counts, color = day)) +
    geom_point(alpha = .7) +
    geom_line() +
    labs(
      title = "24 Hour Accelerometer Activity",
      x = "Minute of the Day",
      y = "Activity Count"
    )
```

This graph shows the accelermeter activity count across the 24 hours of a each day. Although it may be a little hard to see, this graph shows us that there is the least amount of activity count throughout the night which makes sense because they are most likely sleeping/resting during this period of time. Additionally, there seems to be a jump in activity count right before this period of low activity count during the night. Other periods of high activity can be seen during noon on Sundays.

## Problem 3

*Load the NY NOAA dataset and describe the dataset*
```{r, ny_noaa}
data("ny_noaa")
```
This dataset shows the information from NOAA for the state of New York from January 1, 1981 through December 31, 2010. There are **`r nrow(ny_noaa)` observations** and **`r ncol(ny_noaa)` variables**. It contains the variables: ID which corresponds to the different weather stations, date of observation, precipitation (tenths of mm), snowfall (mm), snow depth (mm), maximum temperature (tenths of C) and minimum temperature (tenths of C). 

*Clean data: separate variables for year, month, day; give reasonable units for variables.*
```{r, clean_data}
clean_ny_noaa =   
  ny_noaa %>% 
  separate(date, sep = "-", into = c("year", "month", "day")) %>% 
  mutate(
    prcp = (prcp / 10), # convert units to mm
    tmin = as.integer(tmin) / 10, # convert units to C
    tmax = as.integer(tmax) / 10 # convert units to C
  )
```

*Most commonly observed values for snowfall*
```{r snowfall}
snowfall =    
  clean_ny_noaa %>% 
    count(snow) %>% 
    arrange(desc(n)) 
```

This dataset now has precipitation converted to mm and the maximum and minimum temperature variables are converted to degrees C. The most commonly observed snowfall values are 0 because this dataset contains observations throughout the entire year and New York State only experiences snow in the winter. Next we have "NA" or missing values as the next most common because many station do not have records for snowfall, and third is 25 mm.

*Two-panel plot showing average max temperature in January and July in each station across years*
```{r, jan_jul_plot, message = FALSE, warning = FALSE}
clean_ny_noaa %>% 
  group_by(id, year, month) %>% 
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE)
  ) %>% 
  filter(month %in% c("01", "07")) %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id)) +
  geom_point(alpha = .5) +
  geom_path() +
  facet_grid(. ~ month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        legend.position = "none")

```

The graph shows each station's recorded mean maximum temperature throughout the years from 1981 to 2010. As we can see from the plots, the month of January has much lower mean temperatures (ranging from about -15 to 10 degrees C) compared to the July (ranging from about 20 to 30 degrees C with some outliers). Overall, the mean maximum temperatures seem to be very consistent over the 30 year period. Winter temperatures have a wider range of mean values than compared to the summer temperatures.

*Two-panel plot showing tmax vs tmin for full dataset and distribution of snowfall values greater than 0 and less than 100 by year*
```{r, patchwork_plot, collapse = TRUE, warning = FALSE, message = FALSE}
tmax_tmin =
  clean_ny_noaa %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_density2d() +
    labs(
      title = "Maximum vs Minimum Temperatures in NYS",
      x = "Maximum Temperature",
      y = "Minimum Temperature"
    )

distr_snowfall = 
  clean_ny_noaa %>% 
  filter(snow > 0, snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
    labs(
      title = "Distribution of Snowfall (0-100mm) by Year",
      x = "Year",
      y = "Snowfall"
    ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
  
tmax_tmin / distr_snowfall
```

This plot on the top shows the maximum vs. minimum temperatures of all the stations throughout the full time period. As we can see, the highest density areas are around when both the minimum and maximum temperatures are around 0 degrees Celsius and around 20-30 degrees Celsius.

The bottom plot shows us the distribution of snowfall values greater than 0 and less than 100mm by year. Based on the boxplots for each year we can see that the distribution of snowfall seems to be very stable throughout this entire time period, the median snowfall for this plot is 25mm. 


